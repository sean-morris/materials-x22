{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc98f7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab03.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c468260",
   "metadata": {},
   "source": [
    "# Lab 3: Movie Classification: Part 1\n",
    "\n",
    "Welcome to Lab 3!\n",
    "This lab is split into two parts: Lab 3 and 4. You will build a classification model that guesses whether a movie is a comedy or a thriller by using only the number of times chosen words appear in the movie's screenplay. By the end of this lab and the next, you should know how to:\n",
    "\n",
    "1. Build a k-nearest-neighbors classifier.\n",
    "2. Test a classifier on data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d06ff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up the notebook, but please don't change it.\n",
    "import numpy as np\n",
    "import math\n",
    "import datascience \n",
    "from datascience import *\n",
    "\n",
    "# These lines set up the plotting functionality and formatting.\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77275e5e",
   "metadata": {},
   "source": [
    "# Part 1: The Dataset\n",
    "\n",
    "In this project, we are exploring movie screenplays. We'll be trying to predict each movie's genre from the text of its screenplay. In particular, we have compiled a list of 5,000 words that occur in conversations between movie characters. For each movie, our dataset tells us the frequency with which each of these words occurs in certain conversations in its screenplay. All words have been converted to lowercase.\n",
    "\n",
    "Run the cell below to read the `movies` table. **It may take up to a minute to load.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7cb65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = Table.read_table('movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d44df",
   "metadata": {},
   "source": [
    "Here is one row of the table and some of the frequencies of words that were said in the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.where(\"Title\", \"runaway bride\").select(0, 1, 2, 3, 4, 14, 49, 1042, 4004)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691ec5c",
   "metadata": {},
   "source": [
    "The above cell prints a few columns of the row for the comedy movie *Runaway Bride*.  The movie contains 4895 words. The word \"it\" appears 115 times, as it makes up  $\\frac{115}{4895} \\approx 0.0234092$ of the words in the movie. The word \"england\" doesn't appear at all.\n",
    "\n",
    "Additional context: This numerical representation of a body of text, one that describes only the frequencies of individual words, is called a bag-of-words representation. This is a model that is often used in [NLP](https://en.wikipedia.org/wiki/Natural_language_processing). A lot of information is discarded in this representation: the order of the words, the context of each word, who said what, the cast of characters and actors, etc. However, a bag-of-words representation is often used for machine learning applications as a reasonable starting point, because a great deal of information is also retained and expressed in a convenient and compact format. \n",
    "\n",
    "In this project, we will investigate whether this representation is sufficient to build an accurate genre classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e707f88",
   "metadata": {},
   "source": [
    "All movie titles are unique. The `row_for_title` function provides fast access to the one row for each title. \n",
    "\n",
    "*Note: All movies in our dataset have their titles lower-cased.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_index = movies.index_by('Title')\n",
    "def row_for_title(title):\n",
    "    \"\"\"Return the row for a title, similar to the following expression (but faster)\n",
    "    \n",
    "    movies.where('Title', title).row(0)\n",
    "    \"\"\"\n",
    "    return title_index.get(title)[0]\n",
    "\n",
    "row_for_title('toy story')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49190b",
   "metadata": {},
   "source": [
    "For example, the fastest way to find the frequency of \"fun\" in the movie *Toy Story* is to access the `'fun'` item from its row. Check the original table to see if this worked for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_for_title('toy story').item('fun') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4f9f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 1.0\n",
    "Set `expected_row_sum` to the number that you __expect__ will result from summing all proportions in each row, excluding the first five columns. Think about what any one row adds up to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce8311",
   "metadata": {
    "deletable": false,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Set row_sum to a number that's the (approximate) sum of each row of word proportions.\n",
    "expected_row_sum = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957645ed",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_0 - 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d510e5",
   "metadata": {},
   "source": [
    "This dataset was extracted from [a dataset from Cornell University](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). After transforming the dataset (e.g., converting the words to lowercase, removing the naughty words, and converting the counts to frequencies), we created this new dataset containing the frequency of 5000 common words in each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9404bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Words with frequencies:', movies.drop(np.arange(5)).num_columns) \n",
    "print('Movies with genres:', movies.num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dbe154",
   "metadata": {},
   "source": [
    "## 1.1. Word Stemming\n",
    "The columns other than \"Title\", \"Year\", \"Rating\", \"Genre\", and \"# Words\" in the `movies` table are all words that appear in some of the movies in our dataset.  These words have been *stemmed*, or abbreviated heuristically, in an attempt to make different [inflected](https://en.wikipedia.org/wiki/Inflection) forms of the same base word into the same string.  For example, the column \"manag\" is the sum of proportions of the words \"manage\", \"manager\", \"managed\", and \"managerial\" (and perhaps others) in each movie. This is a common technique used in machine learning and natural language processing.\n",
    "\n",
    "Stemming makes it a little tricky to search for the words you want to use, so we have provided another table called `vocab_table` that will let you see examples of unstemmed versions of each stemmed word. Run the code below to load it.\n",
    "\n",
    "**Note:** You should use `vocab_table` for the rest of Section 1.1, not `vocab_mapping`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec64157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell.\n",
    "vocab_mapping = Table.read_table('stem.csv')\n",
    "stemmed = np.take(movies.labels, np.arange(3, len(movies.labels)))\n",
    "vocab_table = Table().with_column('Stem', stemmed).join('Stem', vocab_mapping)\n",
    "vocab_table.take(np.arange(1100, 1110))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cefd00",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 1.1.1\n",
    "Using `vocab_table`, find the stemmed version of the word \"elements\" and assign the value to `stemmed_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49852a0",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "stemmed_message = ...\n",
    "stemmed_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a6671",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_1_1 - 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb311d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 1.1.2\n",
    "What stem in the dataset has the most words that are shortened to it? Assign `most_stem` to that stem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc223322",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "most_stem = ...\n",
    "most_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4f9f5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_1_2 - 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a034775",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 1.1.3\n",
    "What is the longest word in the dataset whose stem wasn't shortened? Assign that to `longest_uncut`. Break ties alphabetically from Z to A (so if your options are \"cat\" or \"bat\", you should pick \"cat\"). Note that when sorting letters, the letter `a` is smaller than the letter `z`. You may also want to sort more than once.\n",
    "\n",
    "*Hint 1:* `vocab_table` has 2 columns: one for stems and one for the unstemmed (normal) word. Find the longest word that wasn't cut at all (same length as stem).\n",
    "\n",
    "*Hint 2:* There is a table function that allows you to compute a function on every element in a column. Check [Python Reference](https://www.data8.org/sp24/reference/) if you aren't sure which one.\n",
    "\n",
    "*Hint 3:* In our solution, we found it useful to first add columns with the length of the word and the length of the stem, and then to add a column with the difference between those lengths. What will the difference be if the word is not shortened?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816a374",
   "metadata": {
    "for_assignment_type": "student",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "tbl_with_lens = ...\n",
    "tbl_with_diff = ...\n",
    "\n",
    "longest_uncut = ...\n",
    "longest_uncut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd7d67",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_1_3 - 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6105e2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 1.1.4\n",
    "How many stems have only one word that is shortened to them? For example, if the stem \"book\" only maps to the word \"books\" and if the stem \"a\" only maps to the word \"a,\" both should be counted as stems that map only to a single word.\n",
    "\n",
    "Assign `count_single_stems` to the count of stems that map to one word only. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63128b77",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "count_single_stems = ...\n",
    "count_single_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34351a63",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_1_4 - 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fe81f3",
   "metadata": {},
   "source": [
    "## 1.2. Exploratory Data Analysis: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af929fbd",
   "metadata": {},
   "source": [
    "Let's explore our dataset before trying to build a classifier. To start, we’ll use the associated proportions to investigate the relationship between different words.\n",
    "\n",
    "The first association we'll investigate is the association between the proportion of words that are \"outer\" and the proportion of words that are \"space\". \n",
    "\n",
    "As usual, we'll investigate our data visually before performing any numerical analysis.\n",
    "\n",
    "Run the cell below to plot a scatter diagram of \"space\" proportions vs \"outer\" proportions and to create the `outer_space` table. Each point on the scatter plot represents one movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47443a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell!\n",
    "outer_space = movies.select(\"outer\", \"space\")\n",
    "outer_space.scatter(\"outer\", \"space\")\n",
    "plots.axis([-0.0005, 0.001, -0.0005, 0.003]);\n",
    "plots.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec76d8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 1.2.1\n",
    "Looking at that chart it is difficult to see if there is an association. Calculate the correlation coefficient for the potential linear association between proportion of words that are \"outer\" and the proportion of words that are \"space\" for every movie in the dataset, and assign it to `outer_space_r`. \n",
    "\n",
    "*Hint:* If you need a refresher on how to calculate the correlation coefficient check out [Ch 15.1](https://inferentialthinking.com/chapters/15/1/Correlation.html#calculating-r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d2fa60",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# These two arrays should make your code cleaner!\n",
    "outer = movies.column(\"outer\")\n",
    "space = movies.column(\"space\")\n",
    "\n",
    "outer_su = ...\n",
    "space_su = ...\n",
    "\n",
    "outer_space_r = ...\n",
    "outer_space_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a6d453",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_2_1 - 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3b4e4e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 1.2.2\n",
    "Choose two *different* words in the dataset with a magnitude (absolute value) of correlation higher than 0.2 and plot a scatter plot with a line of best fit for them. Please do not pick \"outer\" and \"space\" or \"san\" and \"francisco\". The code to plot the scatter plot and line of best fit is given for you, you just need to calculate the correct values to `r`, `slope` and `intercept`. \n",
    "\n",
    "*Hint 1:* It's easier to think of words with a positive correlation, i.e. words that are often mentioned together. Try to think of common phrases or idioms.\n",
    "\n",
    "*Hint 2:* Refer to [Section 15.2](https://inferentialthinking.com/chapters/15/2/Regression_Line.html#the-equation-of-the-regression-line) of the textbook for the formulas. For additional past examples of regression, see Homework 9.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8af9eaf",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "word_x = ...\n",
    "word_y = ...\n",
    "\n",
    "# These arrays should make your code cleaner!\n",
    "arr_x = movies.column(word_x)\n",
    "arr_y = movies.column(word_y)\n",
    "\n",
    "x_su = ...\n",
    "y_su = ...\n",
    "\n",
    "r = ...\n",
    "\n",
    "slope = ...\n",
    "intercept = ...\n",
    "\n",
    "# DON'T CHANGE THESE LINES OF CODE\n",
    "movies.scatter(word_x, word_y)\n",
    "max_x = max(movies.column(word_x))\n",
    "plots.title(f\"Correlation: {r}, magnitude greater than .2: {abs(r) >= 0.2}\")\n",
    "plots.plot([0, max_x * 1.3], [intercept, intercept + slope * (max_x*1.3)], color='gold');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856fd72a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "#### Question 1.2.3\n",
    "Imagine that you picked the words \"san\" and \"francisco\" as the two words that you would expect to be correlated because they compose the city name San Francisco. Assign `san_francisco` to either the number 1 or the number 2 according to which statement is true regarding the correlation between \"san\" and \"francisco.\" \n",
    "\n",
    "1. \"san\" can also precede other city names like San Diego and San Jose. This might lead to \"san\" appearing in movies without \"francisco,\" and would reduce the correlation between \"san\" and \"francisco.\"\n",
    "2. \"san\" can also precede other city names like San Diego and San Jose. The fact that \"san\" could appear more often in front of different cities and without \"francisco\" would increase the correlation between \"san\" and \"francisco.\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861495d4",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "san_francisco = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae192ff",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_2_3 - 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce68f52",
   "metadata": {},
   "source": [
    "## 1.3. Splitting the dataset\n",
    "Now, we're going to use our `movies` dataset for two purposes.\n",
    "\n",
    "1. First, we want to *train* movie genre classifiers.\n",
    "2. Second, we want to *test* the performance of our classifiers. \n",
    "\n",
    "Hence, we need two different datasets: *training* and *test*. \n",
    "\n",
    "The purpose of a classifier is to classify unseen data that is similar to the training data. The test dataset will help us determine the accuracy of our predictions by comparing the actual genres of the movies with the genres that our classifier predicts. Therefore, we must ensure that there are no movies that appear in both sets. We do so by splitting the dataset randomly. The dataset has already been permuted randomly, so it's easy to split.  We just take the first 85% of the dataset for training and the rest for test. \n",
    "\n",
    "Run the code below (without changing it) to separate the datasets into two tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have defined the proportion of our data\n",
    "# that we want to designate for training as 17/20ths (85%) \n",
    "# and, of our total dataset 3/20ths (15%) of the data is\n",
    "# reserved for testing\n",
    "\n",
    "training_proportion = 17/20\n",
    "\n",
    "num_movies = movies.num_rows\n",
    "num_train = int(num_movies * training_proportion)\n",
    "num_test = num_movies - num_train\n",
    "\n",
    "train_movies = movies.take(np.arange(num_train))\n",
    "test_movies = movies.take(np.arange(num_train, num_movies))\n",
    "\n",
    "print(\"Training: \",   train_movies.num_rows, \";\",\n",
    "      \"Test: \",       test_movies.num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b176c2f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 1.3.1\n",
    "Draw a horizontal bar chart with two bars that show the proportion of Comedy movies in each dataset (`train_movies` and `test_movies`). The two bars should be labeled \"Training\" and \"Test\". Complete the function `comedy_proportion` first; it should help you create the bar chart. \n",
    "\n",
    "*Hint*: Refer to [Section 7.1](https://inferentialthinking.com/chapters/07/1/Visualizing_Categorical_Distributions.html#bar-chart) of the textbook if you need a refresher on bar charts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a8fc28",
   "metadata": {
    "for_assignment_type": "solution",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def comedy_proportion(table):\n",
    "    # Return the proportion of movies in a table that have the comedy genre.\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "# The staff solution took multiple lines.  Start by creating a table.\n",
    "# If you get stuck, think about what sort of table you need for barh to work\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f738096",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Part 2: K-Nearest Neighbors - A Guided Example\n",
    "\n",
    "[K-Nearest Neighbors (k-NN)](https://inferentialthinking.com/chapters/17/1/Nearest_Neighbors.html) is a classification algorithm.  Given some numerical *attributes* (also called *features*) of an unseen example, it decides which category that example belongs to based on its similarity to previously seen examples. Predicting the category of an example is called *labeling*, and the predicted category is also called a *label*.\n",
    "\n",
    "An attribute (feature) we have about each movie is *the proportion of times a particular word appears in the movie*, and the labels are two movie genres: comedy and thriller.  The algorithm requires many previously seen examples for which both the attributes and labels are known: that's the `train_movies` table.\n",
    "\n",
    "To build understanding, we're going to visualize the algorithm instead of just describing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5832aa1",
   "metadata": {},
   "source": [
    "## 2.1. Classifying a movie\n",
    "\n",
    "In k-NN, we classify a movie by finding the `k` movies in the *training set* that are most similar according to the features we choose. We call those movies with similar features the *nearest neighbors*.  The k-NN algorithm assigns the movie to the most common category among its `k` nearest neighbors.\n",
    "\n",
    "Let's limit ourselves to just 2 features for now, so we can plot each movie.  The features we will use are the proportions of the words \"water\" and \"feel\" in the movie.  Taking the movie *Monty Python and the Holy Grail* (in the test set), 0.000804074 of its words are \"water\" and 0.0010721 are \"feel\". This movie appears in the test set, so let's imagine that we don't yet know its genre.\n",
    "\n",
    "First, we need to make our notion of similarity more precise.  **We will say that the *distance* between two movies is the straight-line distance between them when we plot their features on a scatter diagram.**\n",
    "\n",
    "**This distance is called the Euclidean (\"yoo-KLID-ee-un\") distance, whose formula is $\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$.**\n",
    "\n",
    "For example, in the movie *Clerks.* (in the training set), 0.00016293 of all the words in the movie are \"water\" and 0.00154786 are \"feel\".  Its distance from *Monty Python and the Holy Grail* on this 2-word feature set is $\\sqrt{(0.000804074 - 0.000162933)^2 + (0.0010721 - 0.00154786)^2} \\approx 0.000798379$.  (If we included more or different features, the distance could be different.)\n",
    "\n",
    "A third movie, *The Godfather* (in the training set), has 0 \"water\" and 0.00015122 \"feel\". \n",
    "\n",
    "The function below creates a plot to display the \"water\" and \"feel\" features of a test movie and some training movies. As you can see in the result, *Monty Python and the Holy Grail* is more similar to *Clerks.* than to the *The Godfather* based on these features, which makes sense as both movies are comedy movies, while *The Godfather* is a thriller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bff858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell.\n",
    "def plot_with_two_features(test_movie, training_movies, x_feature, y_feature):\n",
    "    \"\"\"Plot a test movie and training movies using two features.\"\"\"\n",
    "    test_row = row_for_title(test_movie)\n",
    "    distances = Table().with_columns(\n",
    "            x_feature, [test_row.item(x_feature)],\n",
    "            y_feature, [test_row.item(y_feature)],\n",
    "            'Color',   ['unknown'],\n",
    "            'Title',   [test_movie]\n",
    "        )\n",
    "    for movie in training_movies:\n",
    "        row = row_for_title(movie)\n",
    "        distances.append([row.item(x_feature), row.item(y_feature), row.item('Genre'), movie])\n",
    "    distances.scatter(x_feature, y_feature, group='Color', labels='Title', s=50)\n",
    "    \n",
    "training = [\"clerks.\", \"the godfather\"] \n",
    "plot_with_two_features(\"monty python and the holy grail\", training, \"water\", \"feel\")\n",
    "plots.axis([-0.0008, 0.001, -0.004, 0.007]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e1137",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 2.1.1\n",
    "\n",
    "Compute the Euclidean distance (defined in the section above) between the two movies, *Monty Python and the Holy Grail* and *The Godfather*, using the `water` and `feel` features only.  Assign it the name `one_distance`. \n",
    "\n",
    "*Hint 1:* If you have a row, you can use `item` to get a value from a column by its name.  For example, if `r` is a row, then `r.item(\"Genre\")` is the value in column `\"Genre\"` in row `r`.\n",
    "\n",
    "*Hint 2:* Refer to the beginning of Part 1 if you don't remember what `row_for_title` does.\n",
    "\n",
    "*Hint 3:* In the formula for Euclidean distance, think carefully about what `x` and `y` represent. Refer to the example in the text above if you are unsure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc538ea7",
   "metadata": {
    "deletable": false,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "python = row_for_title(\"monty python and the holy grail\") \n",
    "godfather = row_for_title(\"the godfather\") \n",
    "\n",
    "one_distance = ...\n",
    "one_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3207c82e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_1_1 - 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ac1f0b",
   "metadata": {},
   "source": [
    "Below, we've added a third training movie, *The Silence of the Lambs*. Before, the point closest to *Monty Python and the Holy Grail* was *Clerks.*, a comedy movie. However, now the closest point is *The Silence of the Lambs*, a thriller movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1fd661",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = [\"clerks.\", \"the godfather\", \"the silence of the lambs\"] \n",
    "plot_with_two_features(\"monty python and the holy grail\", training, \"water\", \"feel\") \n",
    "plots.axis([-0.0008, 0.001, -0.004, 0.007]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db09f6f5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 2.1.2\n",
    "Complete the function `distance_two_features` that computes the Euclidean distance between any two movies, using two features. The last two lines call your function to show that *Monty Python and the Holy Grail* is closer to *The Silence of the Lambs* than it is to *Clerks*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d646ed8",
   "metadata": {
    "deletable": false,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def distance_two_features(title0, title1, x_feature, y_feature):\n",
    "    \"\"\"Compute the distance between two movies with titles title0 and title1.\n",
    "    \n",
    "    Only the features named x_feature and y_feature are used when computing the distance.\n",
    "    \"\"\"\n",
    "    row0 = ...\n",
    "    row1 = ...\n",
    "    ...\n",
    "\n",
    "for movie in make_array(\"clerks.\", \"the silence of the lambs\"):\n",
    "    movie_distance = distance_two_features(movie, \"monty python and the holy grail\", \"water\", \"feel\")\n",
    "    print(movie, 'distance:\\t', movie_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eade3c8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_1_2 - 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f890e1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 2.1.3\n",
    "Define the function `distance_from_python` so that it works as described in its documentation/[docstring](https://www.geeksforgeeks.org/python-docstrings/)(the string right below where the function is defined).\n",
    "\n",
    "\n",
    "**Note:** Your solution should not use arithmetic operations directly. Instead, it should make use of previously defined functions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb849c",
   "metadata": {
    "deletable": false,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def distance_from_python(title):\n",
    "    \"\"\"Return the distance between the given movie and \"monty python and the holy grail\", \n",
    "    based on the features \"water\" and \"feel\".\n",
    "    \n",
    "    This function takes a single argument:\n",
    "      title: A string, the name of a movie.\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "\n",
    "# Calculate the distance between \"Clerks.\" and \"Monty Python and the Holy Grail\"\n",
    "distance_from_python('clerks.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd98f20b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_1_3 - 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7399e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 2.1.4\n",
    "\n",
    "Using the features `water` and `feel`, what are the names and genres of the 5 movies in the **training set** closest to *Monty Python and the Holy Grail*?  To answer this question, make a **table** named `close_movies` containing those 5 movies with columns `\"Title\"`, `\"Genre\"`, `\"water\"`, and `\"feel\"`, as well as a column called `\"distance from python\"` that contains the distance from *Monty Python and the Holy Grail*.  The table should be **sorted in ascending order by `\"distance from python\"`**.\n",
    "\n",
    "*Note:* Why are smaller distances from *Monty Python and the Holy Grail* more helpful in helping us classify the movie?\n",
    "\n",
    "*Hint:* Your final table should only have 5 rows. How can you get the first five rows of a table?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d8913",
   "metadata": {
    "for_assignment_type": "solution",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# The staff solution took multiple lines.\n",
    "...\n",
    "close_movies = ...\n",
    "close_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf7cd1e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_1_4 - 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ef1e4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 2.1.5\n",
    "Next, we'll classify *Monty Python and the Holy Grail* based on the genres of the closest movies. \n",
    "\n",
    "To do so, define the function `most_common` so that it works as described in its documentation below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d88ac",
   "metadata": {
    "deletable": false,
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def most_common(label, table):\n",
    "    \"\"\"This function takes two arguments:\n",
    "      label: The label of a column, a string.\n",
    "      table: A table.\n",
    "     \n",
    "    It returns the most common value in the label column of the table.\n",
    "    In case of a tie, it returns any one of the most common values.    \n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "# Calling most_common on your table of 5 nearest neighbors classifies\n",
    "# \"monty python and the holy grail\" as a thriller movie, 3 votes to 2. \n",
    "most_common('Genre', close_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cd37dd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_1_5 - 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5189fc6c",
   "metadata": {},
   "source": [
    "Congratulations are in order — you've classified your first movie! However, we can see that the classifier doesn't work too well since it categorized *Monty Python and the Holy Grail* as a thriller movie (unless you count the thrilling holy hand grenade scene). In next part, we will try to do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b6198",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93621b76",
   "metadata": {},
   "source": [
    "## Finishing up\n",
    "\n",
    "**Important submission information:** \n",
    "- Be sure to run the tests and verify that they all pass by running the `grader.check_all()` cell below,\n",
    "- Save your progress by choosing the **Save and Checkpoint** item in the **File** menu, \n",
    "- Submit your work by clicking the **Submit** button in the toolbar at the top of notebook. \n",
    "- Download a zip file of this notebook by running the last cell below. **Note:** Be sure to run all the tests before exporting so that all images/graphs appear in the exported notebook. \n",
    "\n",
    "**Please save before submitting!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627bfe80",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05bd399",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ebb53a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea3877",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d86504",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1_0 - 3": {
     "name": "q1_0 - 3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 0 < expected_row_sum\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1_1_1 - 3": {
     "name": "q1_1_1 - 3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (type(stemmed_message) == str) | (type(stemmed_message) == np.str_)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(stemmed_message) < len('elements')\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1_1_2 - 3": {
     "name": "q1_1_2 - 3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (type(most_stem) == str) | (type(most_stem) == np.str_)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1_1_3 - 3": {
     "name": "q1_1_3 - 3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (type(longest_uncut) == str) | (type(longest_uncut) == np.str_)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1_1_4 - 3": {
     "name": "q1_1_4 - 3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(count_single_stems) == int\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1_2_1 - 3": {
     "name": "q1_2_1 - 3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 0.2 < outer_space_r < 0.4\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1_2_3 - 3": {
     "name": "q1_2_3 - 3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(san_francisco) == int\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2_1_1 - 3": {
     "name": "q2_1_1 - 3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 0 < one_distance < 0.01\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2_1_2 - 2": {
     "name": "q2_1_2 - 2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> correct_dis = 0.001406116\n>>> dis = distance_two_features('clerks.', 'the godfather', 'water', 'feel')\n>>> np.isclose(np.round(dis, 9), correct_dis)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> correct_dis = 0.005647119\n>>> dis = distance_two_features('clerks.', 'the godfather', 'your', 'that')\n>>> np.isclose(np.round(dis, 9), correct_dis)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2_1_3 - 3": {
     "name": "q2_1_3 - 3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(distance_from_python('clerks.'), 0.00079838)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2_1_4 - 3": {
     "name": "q2_1_4 - 3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> set(close_movies.labels) >= {'Genre', 'Title', 'feel', 'water'}\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> close_movies.num_rows == 5\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> close_movies.column('Title').item(0) != 'monty python and the holy grail'\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2_1_5 - 3": {
     "name": "q2_1_5 - 3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> [most_common('Genre', close_movies.take(range(k))) for k in range(1, 6, 2)]\n['thriller', 'thriller', 'thriller']",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "otter_service": {
   "assignment": "lab03",
   "course": "8x",
   "section": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
